{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c755eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import statistics\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import neighbors\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import max_error\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import warnings\n",
    "\n",
    "cudnn.benchmark = True\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f9e19",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Data Preprocessing and feature extraction \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57840cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opensmile in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (2.4.1)\n",
      "Requirement already satisfied: audinterface>=0.7.0 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from opensmile) (0.8.1)\n",
      "Requirement already satisfied: audobject>=0.6.1 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from opensmile) (0.7.1)\n",
      "Requirement already satisfied: audformat<2.0.0,>=0.10.1 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audinterface>=0.7.0->opensmile) (0.14.2)\n",
      "Requirement already satisfied: audresample<2.0.0,>=1.1.0 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audinterface>=0.7.0->opensmile) (1.1.0)\n",
      "Requirement already satisfied: pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (1.4.1)\n",
      "Requirement already satisfied: audiofile>=0.4.0 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (1.1.0)\n",
      "Requirement already satisfied: iso-639 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (0.4.5)\n",
      "Requirement already satisfied: audeer<2.0.0,>=1.18.0 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (1.18.0)\n",
      "Requirement already satisfied: oyaml in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (1.0)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (6.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audeer<2.0.0,>=1.18.0->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (4.64.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (1.21.5)\n",
      "Requirement already satisfied: soundfile in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (0.10.3.post1)\n",
      "Requirement already satisfied: sox in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audiofile>=0.4.0->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (1.4.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audobject>=0.6.1->opensmile) (4.11.3)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile) (3.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from python-dateutil>=2.8.1->pandas!=1.3.0,!=1.3.1,!=1.3.2,!=1.3.3,!=1.4.0,>=1.1.5->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from cffi>=1.0->soundfile->audiofile>=0.4.0->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (2.21)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from tqdm->audeer<2.0.0,>=1.18.0->audformat<2.0.0,>=0.10.1->audinterface>=0.7.0->opensmile) (0.4.4)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (6.0)\n",
      "Requirement already satisfied: audiofile in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: sox in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audiofile) (1.4.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audiofile) (1.21.5)\n",
      "Requirement already satisfied: soundfile in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audiofile) (0.10.3.post1)\n",
      "Requirement already satisfied: audeer in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audiofile) (1.18.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from audeer->audiofile) (4.64.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from soundfile->audiofile) (1.15.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from cffi>=1.0->soundfile->audiofile) (2.21)\n",
      "Requirement already satisfied: colorama in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from tqdm->audeer->audiofile) (0.4.4)\n",
      "Requirement already satisfied: xgboost in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from xgboost) (1.21.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\kathi sternbauer\\.conda\\envs\\affective\\lib\\site-packages (from xgboost) (1.7.3)\n"
     ]
    }
   ],
   "source": [
    "# Installing opensmile via pip\n",
    "!pip install opensmile\n",
    "# update required packages if needed\n",
    "!pip install --upgrade pyyaml\n",
    "# install audiofile \n",
    "!pip install audiofile\n",
    "# install XtremeGradientBoost classifier\n",
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18a8d24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import opensmile\n",
    "import audiofile\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3572de43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale x feastures\n",
    "def scale_features(X_train, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "# Train SVM classifier\n",
    "def train_svm(X_train, Y_train, Z_train, svm_kernel='rbf'):\n",
    "    ###########################################################################################\n",
    "    #  OPTIMIZATION THROUGH THE LEAVE-ONE-SPEAKER-OUT METHOD, i.e. (6-FOLD) CROSS-VALIDATION  #\n",
    "    ###########################################################################################\n",
    "\n",
    "    # Define 5 levels of complexity\n",
    "    C = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "    # Create list to collect mean results (across the folds) for each complexity\n",
    "    results = []\n",
    "    # Run experiments for each complexity\n",
    "    for elem in C:\n",
    "        # Define classifier with one-vs-the-rest multi-class strategy\n",
    "        clf = svm.SVC(C=elem, kernel=svm_kernel)\n",
    "        # Create empty list to collect scores for each cv for the given complexity\n",
    "        scores = []\n",
    "        users = np.unique(Z_train)\n",
    "        # Create splitting considering each time a different speaker in test\n",
    "        for speaker_out in users:\n",
    "            data_train   = np.empty((0, X_train.shape[1]))\n",
    "            target_train = np.empty((0, 1))\n",
    "            data_val     = np.empty((0, X_train.shape[1]))\n",
    "            target_val   = np.empty((0, 1))\n",
    "            for index, utterance in enumerate(Z_train):\n",
    "                if utterance == speaker_out:\n",
    "                    data_val   = np.append(data_val, [X_train[index]], axis=0)\n",
    "                    target_val = np.append(target_val, [Y_train[index]], axis=0)\n",
    "                else:\n",
    "                    data_train   = np.append(data_train, [X_train[index]], axis=0)\n",
    "                    target_train = np.append(target_train, [Y_train[index]], axis=0)\n",
    "            # Run svm for each of the 6-fold cross validations\n",
    "            clf.fit(data_train, target_train.ravel())  # add .ravel() to avoid dataConversion warning\n",
    "            predictions = clf.predict(data_val)\n",
    "            UAR = recall_score(target_val, predictions, average='macro')\n",
    "            scores.append(UAR)\n",
    "        # Compute mean of accuracies for the given complexity and append it to the results list\n",
    "        results.append((statistics.mean(scores), elem))\n",
    "    return results\n",
    "\n",
    "#Tune SVM for best C gotten from cross-validation earlier\n",
    "def tune_svm(results, X_train, Y_train, X_test, Y_test, svm_kernel='rbf'):\n",
    "    #############################################\n",
    "    #  PERFORM FINAL TRAINING/TEST WITH BEST C  #\n",
    "    #############################################\n",
    "\n",
    "    # Get best complexity from the cross-validation\n",
    "    best_C = max(results, key=itemgetter(0))[1]\n",
    "\n",
    "    print(f'C: {best_C}')\n",
    "    \n",
    "    # Make training again with the optimal hyper-parameters\n",
    "    clf = svm.SVC(C=best_C, kernel=svm_kernel)\n",
    "    clf.fit(X_train, Y_train.ravel())\n",
    "    # Make final test\n",
    "    predictions = clf.predict(X_test)\n",
    "    UAR = recall_score(Y_test, predictions, average='macro')\n",
    "    print(predictions)\n",
    "    print(UAR)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# print the statistics of the predictions for a given test set (of labels)\n",
    "def print_statistics(Y_test, predictions):\n",
    "    # Compute evaluation metrics\n",
    "    UAR = recall_score(Y_test, predictions, average='macro')\n",
    "    WAR = recall_score(Y_test, predictions, average='weighted')\n",
    "    print('UAR =', UAR)\n",
    "    print('WAR =', WAR)\n",
    "    # Confusion matrix\n",
    "\n",
    "    cm = confusion_matrix(Y_test, predictions, labels=[1,2,3,4,5,6,7,8])\n",
    "    print(cm)\n",
    "\n",
    "    # Compute recall, precision, and F1 score for each class\n",
    "    rec_result = recall_score(Y_test, predictions, average=None, labels=[1,2,3,4,5,6,7,8])\n",
    "    print('Recall for neutral =', rec_result[0]*100, '%')\n",
    "    print('Recall for calm =',  rec_result[1]*100, '%')\n",
    "    print('Recall for happy =', rec_result[2]*100, '%')\n",
    "    print('Recall for sad =', rec_result[3]*100, '%')\n",
    "    print('Recall for angry =', rec_result[4]*100, '%')\n",
    "    print('Recall for fearful =', rec_result[5]*100, '%')\n",
    "    print('Recall for disgust =', rec_result[6]*100, '%')\n",
    "    print('Recall for surprised =', rec_result[7]*100, '%')\n",
    "\n",
    "    prec_result = precision_score(Y_test, predictions, average=None, labels=[1,2,3,4,5,6,7,8])\n",
    "    print('Precision for neutral =', prec_result[0]*100, '%')\n",
    "    print('Precision for calm =',  prec_result[1]*100, '%')\n",
    "    print('Precision for happy =', prec_result[2]*100, '%')\n",
    "    print('Precision for sad =', prec_result[3]*100, '%')\n",
    "    print('Precision for angry =', prec_result[4]*100, '%')\n",
    "    print('Precision for fearful =', prec_result[5]*100, '%')\n",
    "    print('Precision for disgust =', prec_result[6]*100, '%')\n",
    "    print('Precision for surprised =', prec_result[7]*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff33c4f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Task 0: Extract the baseline eGeMAPSv02 feature set \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf38b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature extractor to get functionals from the eGeMAPS (v02) feature set\n",
    "smile_func = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "    feature_level=opensmile.FeatureLevel.Functionals,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a49e82c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n",
      "Finished data extraction phase\n"
     ]
    }
   ],
   "source": [
    "my_dir = os.getcwd()\n",
    "\n",
    "# Collect data\n",
    "for phase in ['test', 'train']:\n",
    "    print(phase)\n",
    "    \n",
    "    functionals = []\n",
    "    \n",
    "    csv_name = my_dir + '/RESULTS/' + phase + '.csv'\n",
    "    labels_ID = {'label': [], 'intensity' : [], 'ID': [], 'name': []}\n",
    "    for file in os.listdir(f'resources/{phase}/'):  \n",
    "        file_name  = os.path.basename(file[0:-4])\n",
    "        speaker_ID = file_name[-2:]\n",
    "        label      = file_name[6:8]\n",
    "        intensity  = file_name[9:11]\n",
    "\n",
    "        labels_ID['label'].append(label)\n",
    "        labels_ID['intensity'].append(intensity)\n",
    "        labels_ID['ID'].append(speaker_ID)\n",
    "        labels_ID['name'].append(file_name)\n",
    "        \n",
    "        functional = smile_func.process_file(f'resources/{phase}/{file}')\n",
    "        functional['file'] = file\n",
    "        functionals.append(functional)\n",
    "    \n",
    "    pd.concat(functionals).to_csv(csv_name, index=False)\n",
    "    df = pd.DataFrame.from_dict(labels_ID)\n",
    "    df.to_csv(my_dir + '/RESULTS/' + phase + '_labels_ID.csv', sep='\\t', index = False)\n",
    "    \n",
    "print(\"Finished data extraction phase\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26da410c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Task 1: Baseline for SER\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5805db2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "1200  instances\n",
      "8  classes:  [1 2 3 4 5 6 7 8]\n",
      "88  features\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "#  DATA IMPORT  #\n",
    "#################\n",
    "\n",
    "my_dir = os.getcwd()\n",
    "\n",
    "features = pd.read_csv(my_dir + '/RESULTS/train.csv', sep=',', index_col=-1)\n",
    "ID_labels = pd.read_csv(my_dir + '/RESULTS/train_labels_ID.csv', sep='\\t')\n",
    "X_train = features.values  # this extracts the values as a numpy array\n",
    "Y_train = ID_labels.loc[:, ['label']].to_numpy()\n",
    "Z_train = ID_labels.loc[:, ['ID']].to_numpy()\n",
    "\n",
    "features = pd.read_csv(my_dir + '/RESULTS/test.csv', sep=',', index_col=-1)\n",
    "ID_labels = pd.read_csv(my_dir + '/RESULTS/test_labels_ID.csv', sep='\\t')\n",
    "X_test = features.values  # this extracts the values as a numpy array\n",
    "Y_test = ID_labels.loc[:, ['label']].to_numpy()\n",
    "\n",
    "# exploring training set\n",
    "print('TRAINING SET')\n",
    "print(X_train.shape[0], ' instances')\n",
    "print(len(np.unique(Y_train)), ' classes: ', np.unique(Y_train))\n",
    "print(X_train.shape[1], ' features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8b6a18",
   "metadata": {},
   "source": [
    "## Feature Scaling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4978e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Feature scaling #\n",
    "####################\n",
    "# Normalize features in the training set\n",
    "X_train, X_test = scale_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b316bc",
   "metadata": {},
   "source": [
    "## Tuning the classifier & making predictions ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75bd62af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.33359375, 0.0001), (0.42890625, 0.001), (0.49453125, 0.01), (0.509375, 0.1), (0.475, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "results = train_svm(X_train, Y_train, Z_train, svm_kernel='linear')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f4e81dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.1\n",
      "[2 2 1 4 2 1 3 1 5 2 3 1 2 2 1 1 2 2 2 4 2 2 1 4 2 2 2 2 2 4 1 4 2 2 2 4 2\n",
      " 2 1 2 2 2 1 4 2 2 1 2 5 2 3 1 5 3 3 1 5 1 3 1 5 2 3 1 5 3 3 7 8 3 3 5 5 6\n",
      " 8 3 8 3 5 3 4 2 1 4 7 2 2 4 5 2 2 1 5 2 1 1 5 1 2 4 5 2 2 7 4 4 4 7 4 4 2\n",
      " 4 5 2 5 5 5 1 8 3 5 7 8 5 5 7 8 5 5 5 5 5 5 3 5 5 5 5 5 5 5 5 5 5 5 3 3 4\n",
      " 7 3 1 6 4 6 1 6 7 3 1 6 6 6 6 3 6 6 3 6 5 6 5 6 5 6 5 6 5 1 8 7 3 7 4 7 5\n",
      " 6 8 7 5 2 8 7 8 2 5 8 5 7 5 7 3 6 5 7 3 3 8 7 8 8 8 4 8 8 3 8 8 8 8 8 8 8\n",
      " 8 7 8 8 8 8 8 8 8 8 8 8 8 6 8 3 8 4]\n",
      "0.49609375\n"
     ]
    }
   ],
   "source": [
    "predictions = tune_svm(results, X_train, Y_train, X_test, Y_test, svm_kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d6021",
   "metadata": {},
   "source": [
    "**Evaluation metrics**\n",
    "\n",
    "We will evalaute our model's results in terms of:\n",
    "\n",
    "- UAR\n",
    "- WAR\n",
    "- Recall per class\n",
    "- Precision per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52541e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAR = 0.49609375\n",
      "WAR = 0.5041666666666667\n",
      "[[ 6  6  2  1  1  0  0  0]\n",
      " [ 5 21  0  6  0  0  0  0]\n",
      " [ 5  2 12  0  8  1  1  3]\n",
      " [ 5 10  0 10  4  0  3  0]\n",
      " [ 1  1  2  0 23  0  2  3]\n",
      " [ 3  0  6  2  5 14  2  0]\n",
      " [ 1  2  4  1  7  2  9  6]\n",
      " [ 0  0  2  2  0  1  1 26]]\n",
      "Recall for neutral = 37.5 %\n",
      "Recall for calm = 65.625 %\n",
      "Recall for happy = 37.5 %\n",
      "Recall for sad = 31.25 %\n",
      "Recall for angry = 71.875 %\n",
      "Recall for fearful = 43.75 %\n",
      "Recall for disgust = 28.125 %\n",
      "Recall for surprised = 81.25 %\n",
      "Precision for neutral = 23.076923076923077 %\n",
      "Precision for calm = 50.0 %\n",
      "Precision for happy = 42.857142857142854 %\n",
      "Precision for sad = 45.45454545454545 %\n",
      "Precision for angry = 47.91666666666667 %\n",
      "Precision for fearful = 77.77777777777779 %\n",
      "Precision for disgust = 50.0 %\n",
      "Precision for surprised = 68.42105263157895 %\n"
     ]
    }
   ],
   "source": [
    "print_statistics(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7738d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Task 2: Improvement over basemodel\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7132ff",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "eGeMAPSv02 feature + RBF kernel\n",
    "</div>\n",
    "\n",
    "As we have now our baseline model, we would first of like to evaluate the eGeMAPSv02 feature set with a RBF-kernel for our SVM instead of a linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ac8d1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.37890625, 0.0001), (0.37890625, 0.001), (0.37890625, 0.01), (0.396875, 0.1), (0.48828125, 1.0)]\n"
     ]
    }
   ],
   "source": [
    "results = train_svm(X_train, Y_train, Z_train, svm_kernel='rbf')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca11190a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 1.0\n",
      "[2 2 1 4 2 1 3 1 3 2 3 4 2 2 1 4 2 2 1 4 2 2 1 4 2 2 1 2 2 4 2 2 2 2 2 2 2\n",
      " 2 4 2 2 2 1 2 2 2 1 2 1 2 8 1 3 3 3 2 3 1 3 1 7 2 3 4 5 3 3 3 8 3 6 3 3 3\n",
      " 8 3 5 3 8 6 4 2 2 2 7 2 2 4 7 2 2 4 7 2 1 1 4 4 1 4 5 4 4 7 4 4 4 4 5 4 4\n",
      " 4 5 7 5 3 5 1 8 3 5 7 8 3 5 5 8 3 5 3 5 5 5 3 5 3 5 5 5 3 5 5 5 5 7 3 1 4\n",
      " 7 3 1 6 3 2 4 7 3 6 1 6 3 6 3 6 6 6 3 6 5 6 3 6 5 6 3 6 6 3 8 7 6 7 6 7 4\n",
      " 4 1 3 5 3 1 7 5 3 8 4 5 7 8 7 5 6 8 7 5 3 8 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 7 8 7 8 8]\n",
      "0.50390625\n"
     ]
    }
   ],
   "source": [
    "predictions = tune_svm(results, X_train, Y_train, X_test, Y_test, svm_kernel='rbf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ad266ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAR = 0.50390625\n",
      "WAR = 0.5208333333333334\n",
      "[[ 4  6  3  3  0  0  0  0]\n",
      " [ 5 23  0  4  0  0  0  0]\n",
      " [ 4  3 15  1  2  2  1  4]\n",
      " [ 3  8  0 15  2  0  4  0]\n",
      " [ 1  0  8  0 18  0  2  3]\n",
      " [ 3  1  9  2  2 12  3  0]\n",
      " [ 2  0  5  3  5  4  8  5]\n",
      " [ 0  0  0  0  0  0  2 30]]\n",
      "Recall for neutral = 25.0 %\n",
      "Recall for calm = 71.875 %\n",
      "Recall for happy = 46.875 %\n",
      "Recall for sad = 46.875 %\n",
      "Recall for angry = 56.25 %\n",
      "Recall for fearful = 37.5 %\n",
      "Recall for disgust = 25.0 %\n",
      "Recall for surprised = 93.75 %\n",
      "Precision for neutral = 18.181818181818183 %\n",
      "Precision for calm = 56.09756097560976 %\n",
      "Precision for happy = 37.5 %\n",
      "Precision for sad = 53.57142857142857 %\n",
      "Precision for angry = 62.06896551724138 %\n",
      "Precision for fearful = 66.66666666666666 %\n",
      "Precision for disgust = 40.0 %\n",
      "Precision for surprised = 71.42857142857143 %\n"
     ]
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "print_statistics(Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d116b8a",
   "metadata": {},
   "source": [
    "While quite simple and fast to both implement and compute, it does not bring any real advantage, apart from a slightly raised WAR. For recall and precision, some areas are better, yet a lot are worse. Therefore eGeMAPSv02 feature + RBF kernel do not bring any advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2765857",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "ComParE_2016 feature set + SVMs\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9baac5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    " # New opensmile feature extraction in here\n",
    "    # Define feature extractor to get functionals from the eGeMAPS (v02) feature set\n",
    "smile_func = opensmile.Smile(\n",
    "    feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "    feature_level=opensmile.FeatureLevel.Functionals,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b375fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n",
      "Finished data extraction phase 2\n"
     ]
    }
   ],
   "source": [
    "my_dir = os.getcwd()\n",
    "\n",
    "# Collect data\n",
    "for phase in ['test', 'train']:\n",
    "    print(phase)\n",
    "    \n",
    "    functionals = []\n",
    "    \n",
    "    csv_name = my_dir + '/RESULTS/' + phase + '2.csv'\n",
    "    labels_ID = {'label': [], 'intensity' : [], 'ID': [], 'name': []}\n",
    "    for file in os.listdir(f'resources/{phase}/'):  \n",
    "        file_name  = os.path.basename(file[0:-4])\n",
    "        speaker_ID = file_name[-2:]\n",
    "        label      = file_name[6:8]\n",
    "        intensity  = file_name[9:11]\n",
    "\n",
    "        labels_ID['label'].append(label)\n",
    "        labels_ID['intensity'].append(intensity)\n",
    "        labels_ID['ID'].append(speaker_ID)\n",
    "        labels_ID['name'].append(file_name)\n",
    "        \n",
    "        functional = smile_func.process_file(f'resources/{phase}/{file}')\n",
    "        functional['file'] = file\n",
    "        functionals.append(functional)\n",
    "    \n",
    "    pd.concat(functionals).to_csv(csv_name, index=False)\n",
    "    df = pd.DataFrame.from_dict(labels_ID)\n",
    "    df.to_csv(my_dir + '/RESULTS/' + phase + '2_labels_ID.csv', sep='\\t', index = False)\n",
    "    \n",
    "print(\"Finished data extraction phase 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "623b0ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING SET\n",
      "1200  instances\n",
      "8  classes:  [1 2 3 4 5 6 7 8]\n",
      "6373  features\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "#  DATA IMPORT  #\n",
    "#################\n",
    "\n",
    "my_dir = os.getcwd()\n",
    "\n",
    "features = pd.read_csv(my_dir + '/RESULTS/train2.csv', sep=',', index_col=-1)\n",
    "ID_labels = pd.read_csv(my_dir + '/RESULTS/train2_labels_ID.csv', sep='\\t')\n",
    "X_train = features.values  # this extracts the values as a numpy array\n",
    "Y_train = ID_labels.loc[:, ['label']].to_numpy()\n",
    "Z_train = ID_labels.loc[:, ['ID']].to_numpy()\n",
    "\n",
    "features = pd.read_csv(my_dir + '/RESULTS/test2.csv', sep=',', index_col=-1)\n",
    "ID_labels = pd.read_csv(my_dir + '/RESULTS/test2_labels_ID.csv', sep='\\t')\n",
    "X_test = features.values  # this extracts the values as a numpy array\n",
    "Y_test = ID_labels.loc[:, ['label']].to_numpy()\n",
    "\n",
    "# exploring training set\n",
    "print('TRAINING SET')\n",
    "print(X_train.shape[0], ' instances')\n",
    "print(len(np.unique(Y_train)), ' classes: ', np.unique(Y_train))\n",
    "print(X_train.shape[1], ' features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a08fde2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Feature scaling #\n",
    "####################\n",
    "X_train, X_test = scale_features(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc048cfe",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Idea: Use grid search over the SVM over ComParE_2016 feature set with GridSearchCV\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d11ee89",
   "metadata": {},
   "source": [
    "Another idea to find the best possible SVM classifier, apart from simply trying out several combinations by hand, is doing exhaustive search (for hyperparameter training) over out Leave-One-Out SVM. More specifically, we would like to find the most favourable combinations of C and Gamma with respect to the used kernel. As the last controlled parameter, we choose both balanced and non-optimized class weights for each combintation of C, gamma and kernel.\n",
    "\n",
    "- C -> Is a regularization parameter\n",
    "    - Higher C -> Avoid as much missclassification as possible, but smaller-margin hyperplane.\n",
    "    - Smaller C -> Aim for wider-margin hyperplance, but allow also more missclassifications\n",
    "- Gamma -> Only affects the outcome for RBF kernels, for other kernels this parameter will be ignored mostly\n",
    "    - Small Gamma -> The influence of a training sample \"reaches far\", therefore has more influence on the decision boundary even when that is far away. This might lead to SVMs that cannot capture the complexity of data and might only separate rather poorly\n",
    "    - High Gamma -> The influence of a training does only reach rather close to the sample, but it has a strong influence.This might lead to overfitting, no matter how we choose our C value\n",
    "   \n",
    "For more about RFB SVM parameters, look here: <https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "411b6a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "    'class_weight': ['balanced', None],\n",
    "    'gamma': [1, 0.1, 0.01, 0.001],\n",
    "    'kernel': ['linear', 'rbf', 'sigmoid'],\n",
    "}\n",
    "\n",
    "def train_SVM_grid(x_train,y_train,param_grid):\n",
    "    tree = svm.SVC(random_state=55)\n",
    "    model = GridSearchCV(tree,param_grid=param_grid, cv=6, n_jobs=-1, verbose=3)\n",
    "    model.fit(x_train,y_train)\n",
    "    print(f'{model.best_params_}\\n{model.best_estimator_}')\n",
    "    return model.best_params_,model.best_estimator_\n",
    "\n",
    "def predict_SVM_grid(model, x_train, y_train, x_test, y_test):\n",
    "    # Make training again with the optimal hyper-parameters\n",
    "    clf = svm.SVC(C=model.C, kernel=model.kernel, gamma=model.gamma, class_weight=model.class_weight)\n",
    "    clf.fit(X_train, y_train.ravel())\n",
    "    # Make final test\n",
    "    predictions = clf.predict(x_test)\n",
    "    WAR = recall_score(y_test, predictions, average='weighted')\n",
    "    print(predictions)\n",
    "    print(WAR)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42fe43b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 6 folds for each of 120 candidates, totalling 720 fits\n",
      "{'C': 0.001, 'class_weight': 'balanced', 'gamma': 1, 'kernel': 'linear'}\n",
      "SVC(C=0.001, class_weight='balanced', gamma=1, kernel='linear', random_state=55)\n"
     ]
    }
   ],
   "source": [
    "svm_param, svm_model = train_SVM_grid(X_train, Y_train.ravel(), param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a49c19a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 7 1 5 4 3 2 1 4 4 2 5 1 2 4 2 4 4 2 7 2 2 2 7 2 2 4 7 4 2 2 2 2 2\n",
      " 2 2 2 2 2 7 2 2 2 2 2 1 3 8 7 3 3 6 2 3 3 8 1 3 4 8 3 5 6 8 4 5 3 8 3 3 3\n",
      " 8 3 3 3 8 3 4 2 2 7 7 2 2 2 4 7 7 4 4 5 7 4 3 4 7 3 3 4 7 7 4 4 7 4 4 4 2\n",
      " 7 5 5 8 5 5 5 8 5 5 5 8 5 5 5 8 5 5 5 6 5 5 5 3 7 5 5 5 5 5 5 5 5 5 6 5 6\n",
      " 7 6 6 6 7 3 1 6 7 3 1 5 6 6 5 6 6 6 3 6 3 6 8 6 5 6 6 3 5 3 7 7 7 7 7 7 5\n",
      " 5 7 7 5 7 7 7 7 7 7 7 5 7 7 7 7 7 7 7 5 7 8 7 8 8 8 8 8 8 8 8 8 8 8 6 5 8\n",
      " 8 8 5 8 8 8 3 3 8 8 8 6 8 6 5 8 8 8]\n",
      "0.6083333333333333\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_SVM_grid(svm_model, X_train, Y_train.ravel(), X_test, Y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86bfeaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAR = 0.59765625\n",
      "WAR = 0.6083333333333333\n",
      "[[ 7  2  1  3  2  0  1  0]\n",
      " [ 0 23  0  5  0  0  4  0]\n",
      " [ 2  1 15  2  2  2  1  7]\n",
      " [ 0  6  3 12  1  0 10  0]\n",
      " [ 0  0  1  0 25  1  1  4]\n",
      " [ 2  0  5  0  5 16  3  1]\n",
      " [ 0  0  1  0  6  0 24  1]\n",
      " [ 0  0  2  0  3  3  0 24]]\n",
      "Recall for neutral = 43.75 %\n",
      "Recall for calm = 71.875 %\n",
      "Recall for happy = 46.875 %\n",
      "Recall for sad = 37.5 %\n",
      "Recall for angry = 78.125 %\n",
      "Recall for fearful = 50.0 %\n",
      "Recall for disgust = 75.0 %\n",
      "Recall for surprised = 75.0 %\n",
      "Precision for neutral = 63.63636363636363 %\n",
      "Precision for calm = 71.875 %\n",
      "Precision for happy = 53.57142857142857 %\n",
      "Precision for sad = 54.54545454545454 %\n",
      "Precision for angry = 56.81818181818182 %\n",
      "Precision for fearful = 72.72727272727273 %\n",
      "Precision for disgust = 54.54545454545454 %\n",
      "Precision for surprised = 64.86486486486487 %\n"
     ]
    }
   ],
   "source": [
    "print_statistics(Y_test.ravel(), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243ffbe4",
   "metadata": {},
   "source": [
    "ComParE_2016 feature + linear kernel seems to be a very good combination at it outperforms the baseline in all recall values and all but two precision values. The UAR went up by a solid 10 percent and WAR bei 8 percent. Overall a decent result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52b0f17",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Gradient boosted classification via grid search\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bfaa2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'criterion': ['friedman_mse', 'squared_error'],\n",
    "    'loss': ['deviance', 'exponential'],\n",
    "    'max_depth': range(1,10),\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "}\n",
    "\n",
    "def train_dec_tree(x_train,y_train,param_grid):\n",
    "    tree = GradientBoostingClassifier(random_state=55)\n",
    "    model = GridSearchCV(tree,param_grid=param_grid, n_jobs=-1, verbose=3)\n",
    "    model.fit(x_train,y_train)\n",
    "    print(f'{model.best_params_}\\n{model.best_estimator_}')\n",
    "    return model.best_params_,model.best_estimator_\n",
    "\n",
    "def predict_dec_tree(model, x_train, y_train, x_test, y_test):\n",
    "    classifier = GradientBoostingClassifier(criterion=model.criterion, max_depth=model.max_depth, max_features=model.max_features)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    predictions = classifier.predict(x_test)\n",
    "    WAR = recall_score(y_test, predictions, average='weighted')\n",
    "    print(predictions)\n",
    "    print(WAR)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6fc39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n",
      "{'criterion': 'squared_error', 'loss': 'deviance', 'max_depth': 3, 'max_features': 'sqrt'}\n",
      "GradientBoostingClassifier(criterion='squared_error', max_features='sqrt',\n",
      "                           random_state=55)\n"
     ]
    }
   ],
   "source": [
    "tree_params, tree_model = train_dec_tree(X_train, Y_train.ravel(), param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f2ff25e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 2 2 4 1 3 4 4 2 3 4 4 4 7 4 2 2 2 2 2 2 2 2 2 2 7 2 2 2 7 2 2 2 2 2 2\n",
      " 2 7 2 7 2 7 2 2 2 7 2 4 4 8 7 4 3 3 4 5 3 3 4 3 4 8 4 5 6 8 3 8 3 8 3 5 3\n",
      " 8 3 5 3 8 3 7 4 2 2 7 4 2 2 7 2 1 2 7 2 2 2 5 4 7 3 7 4 7 7 7 4 7 6 8 4 7\n",
      " 7 5 7 8 7 5 7 8 3 5 7 8 5 5 5 8 5 5 5 5 3 5 5 5 7 5 5 5 5 5 5 5 7 7 4 7 6\n",
      " 7 6 6 7 7 3 1 4 7 3 1 7 5 6 3 6 6 3 3 3 5 6 3 6 5 6 3 3 5 3 7 7 5 7 7 7 5\n",
      " 7 7 3 5 7 7 5 7 7 7 7 5 7 7 7 7 7 7 7 7 7 7 7 8 8 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 5 8 8 8 3 8 8 8 8 8 8]\n",
      "0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_dec_tree(tree_model, X_train, Y_train.ravel(), X_test, Y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "94bfe2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAR = 0.50390625\n",
      "WAR = 0.5333333333333333\n",
      "[[ 1  4  2  7  0  0  2  0]\n",
      " [ 0 26  0  0  0  0  6  0]\n",
      " [ 0  0 12  7  4  1  1  7]\n",
      " [ 1  9  1  6  1  1 12  1]\n",
      " [ 0  0  2  0 20  0  6  4]\n",
      " [ 2  0  9  2  3  9  7  0]\n",
      " [ 0  0  2  0  6  0 24  0]\n",
      " [ 0  0  1  0  1  0  0 30]]\n",
      "Recall for neutral = 6.25 %\n",
      "Recall for calm = 81.25 %\n",
      "Recall for happy = 37.5 %\n",
      "Recall for sad = 18.75 %\n",
      "Recall for angry = 62.5 %\n",
      "Recall for fearful = 28.125 %\n",
      "Recall for disgust = 75.0 %\n",
      "Recall for surprised = 93.75 %\n",
      "Precision for neutral = 25.0 %\n",
      "Precision for calm = 66.66666666666666 %\n",
      "Precision for happy = 41.37931034482759 %\n",
      "Precision for sad = 27.27272727272727 %\n",
      "Precision for angry = 57.14285714285714 %\n",
      "Precision for fearful = 81.81818181818183 %\n",
      "Precision for disgust = 41.37931034482759 %\n",
      "Precision for surprised = 71.42857142857143 %\n"
     ]
    }
   ],
   "source": [
    "print_statistics(Y_test.ravel(), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c51d0",
   "metadata": {},
   "source": [
    "Sadly, the outcome for the best model is not as good as for SVMs. It was an interesting idea to try out gradient boosted trees as the classifier, but it could not increase the accuracy much over the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db393ea",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Try some Random Forrests with GridSeach\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c8b1f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_forest = {\n",
    "    'class_weight': [None, \"balanced\"],\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'max_depth': range(5,20),\n",
    "    'max_features': [None, \"sqrt\"],\n",
    "}\n",
    "\n",
    "def train_random_forest(x_train,y_train,param_grid):\n",
    "    random_forrest = RandomForestClassifier(random_state=55)\n",
    "    model = GridSearchCV(random_forrest,param_grid=param_grid, n_jobs = -1, verbose=3)\n",
    "    model.fit(x_train,y_train)\n",
    "    print(f'{model.best_params_}\\n{model.best_estimator_}')\n",
    "    return model.best_params_,model.best_estimator_\n",
    "\n",
    "def predict_random_forest(model, x_train, y_train, x_test, y_test):\n",
    "    classifier = RandomForestClassifier(criterion=model.criterion, max_depth=model.max_depth, max_features=model.max_features, class_weight=model.class_weight)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    predictions = classifier.predict(x_test)\n",
    "    WAR = recall_score(y_test, predictions, average='weighted')\n",
    "    print(predictions)\n",
    "    print(WAR)\n",
    "    \n",
    "    return predictions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f7da44e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n",
      "{'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 11, 'max_features': None}\n",
      "RandomForestClassifier(class_weight='balanced', max_depth=11, max_features=None,\n",
      "                       random_state=55)\n"
     ]
    }
   ],
   "source": [
    "forest_params, forest_model = train_random_forest(X_train, Y_train.ravel(), param_grid_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29b5aa20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 7 2 7 1 3 1 3 2 3 2 3 2 5 4 2 2 2 2 2 2 2 2 2 2 7 2 2 2 7 2 2 2 2 2 2\n",
      " 2 2 2 7 2 7 2 2 2 2 2 7 4 8 1 3 3 8 7 3 3 3 3 5 3 8 3 3 8 8 3 8 6 8 3 3 3\n",
      " 8 3 8 3 8 3 7 2 2 2 7 2 2 2 7 2 5 2 7 2 7 2 5 4 7 7 7 6 7 7 7 4 7 7 7 6 7\n",
      " 2 5 5 8 5 5 7 8 3 5 5 8 5 5 7 8 7 5 3 5 5 5 3 5 7 5 5 5 7 5 5 5 7 5 6 8 7\n",
      " 7 6 1 8 7 6 3 5 7 6 3 7 7 6 3 6 5 6 3 6 5 6 8 6 3 6 3 3 5 3 7 7 7 7 7 7 5\n",
      " 7 7 7 5 3 7 7 7 7 7 8 7 7 7 7 7 7 7 7 7 7 7 7 8 6 8 8 8 8 8 8 8 8 8 8 8 8\n",
      " 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8]\n",
      "0.5625\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_random_forest(forest_model, X_train, Y_train.ravel(), X_test, Y_test.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f1d1860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAR = 0.53515625\n",
      "WAR = 0.5625\n",
      "[[ 2  5  4  1  1  0  3  0]\n",
      " [ 0 28  0  0  0  0  4  0]\n",
      " [ 1  0 16  1  1  1  2 10]\n",
      " [ 0 11  0  2  2  2 15  0]\n",
      " [ 0  0  3  0 19  0  6  4]\n",
      " [ 1  0  7  0  4 11  6  3]\n",
      " [ 0  0  2  0  3  0 26  1]\n",
      " [ 0  0  0  0  0  1  0 31]]\n",
      "Recall for neutral = 12.5 %\n",
      "Recall for calm = 87.5 %\n",
      "Recall for happy = 50.0 %\n",
      "Recall for sad = 6.25 %\n",
      "Recall for angry = 59.375 %\n",
      "Recall for fearful = 34.375 %\n",
      "Recall for disgust = 81.25 %\n",
      "Recall for surprised = 96.875 %\n",
      "Precision for neutral = 50.0 %\n",
      "Precision for calm = 63.63636363636363 %\n",
      "Precision for happy = 50.0 %\n",
      "Precision for sad = 50.0 %\n",
      "Precision for angry = 63.33333333333333 %\n",
      "Precision for fearful = 73.33333333333333 %\n",
      "Precision for disgust = 41.935483870967744 %\n",
      "Precision for surprised = 63.26530612244898 %\n"
     ]
    }
   ],
   "source": [
    "print_statistics(Y_test.ravel(), predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5abcc69",
   "metadata": {},
   "source": [
    "Random forest do generate a favourable improvement over the baseline and achieve an 5% improvement for UAR and nearly 8% for WAR. However, they were computationally more expensive that our baseline and cannot reach the equally expensive SVM gridsear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d2a37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Extreme Gradient Boosting\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb8e6eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_xgb_classifier(x_train, y_train, x_test, y_test):\n",
    "    xgb = XGBClassifier()\n",
    "    xgb.fit(x_train, y_train)\n",
    "    predictions = xgb.predict(x_test)\n",
    "    WAR = recall_score(y_test, predictions, average='weighted')\n",
    "    print(predictions)\n",
    "    print(WAR)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "058d90d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 1 0 3 3 0 4 3 2 3 4 3 3 1 6 3 1 1 6 1 1 1 6 1 1 1 6 1 1 1 6 1 1 1 1 1 1\n",
      " 1 6 1 1 3 6 1 1 1 6 1 1 3 7 6 2 2 7 3 2 2 7 2 4 1 7 3 4 2 7 2 4 2 7 2 2 3\n",
      " 7 2 4 2 7 5 6 3 1 1 6 3 1 1 6 1 0 1 6 6 1 1 4 3 6 6 6 3 6 3 6 6 6 6 3 3 6\n",
      " 6 4 6 7 4 4 6 7 2 4 4 7 4 4 4 4 6 4 2 4 4 4 4 4 4 4 4 4 4 4 4 4 6 4 5 6 5\n",
      " 6 5 0 7 6 2 2 5 6 5 0 5 6 5 2 2 5 5 4 5 4 5 7 5 4 5 2 2 4 6 6 6 4 6 6 6 4\n",
      " 6 6 6 6 6 6 6 6 6 6 6 4 6 6 6 6 6 6 4 6 6 6 6 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n",
      " 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7]\n",
      "0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "trans_Y_train = le.fit_transform(Y_train)\n",
    "trans_Y_test = le.fit_transform(Y_test)\n",
    "predictions = predict_xgb_classifier(X_train, trans_Y_train, X_test, trans_Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c153a164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAR = 0.5546875\n",
      "WAR = 0.5833333333333334\n",
      "[[24  0  1  0  0  7  0  0]\n",
      " [ 2 12  4  4  1  1  8  0]\n",
      " [ 8  0  7  1  0 15  0  0]\n",
      " [ 0  2  0 23  0  4  3  0]\n",
      " [ 0  6  0  4 13  5  2  0]\n",
      " [ 0  0  0  5  0 27  0  0]\n",
      " [ 0  0  0  0  0  0 32  0]\n",
      " [ 0  0  0  0  0  0  0  0]]\n",
      "Recall for neutral = 75.0 %\n",
      "Recall for calm = 37.5 %\n",
      "Recall for happy = 21.875 %\n",
      "Recall for sad = 71.875 %\n",
      "Recall for angry = 40.625 %\n",
      "Recall for fearful = 84.375 %\n",
      "Recall for disgust = 100.0 %\n",
      "Recall for surprised = 0.0 %\n",
      "Precision for neutral = 66.66666666666666 %\n",
      "Precision for calm = 57.14285714285714 %\n",
      "Precision for happy = 36.84210526315789 %\n",
      "Precision for sad = 57.49999999999999 %\n",
      "Precision for angry = 92.85714285714286 %\n",
      "Precision for fearful = 45.0 %\n",
      "Precision for disgust = 71.11111111111111 %\n",
      "Precision for surprised = 0.0 %\n"
     ]
    }
   ],
   "source": [
    "print_statistics(trans_Y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d79898e",
   "metadata": {},
   "source": [
    "This rather simple to use method performs considerably better than our baseline, but does not yet reach the performance level of the best SVM methods. Therefore we consider it to be a viable idea, yet with room for improvement and definitly not the best. A positive aspect is their relativly low computational time, measureable in mere seconds instead of minutes and hours for our GridSearches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702c271",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Try a CNN \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09b1e862",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test': 240}\n",
      "['angry', 'calm', 'disgust', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_transforms = {\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Grayscale(),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.repeat(3,1,1)),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = './resources/librosa/data/librosa/' # path to iamge data\n",
    "image_datasets = {'test': torchvision.datasets.ImageFolder(os.path.join(data_dir, 'test'), data_transforms['test'])}\n",
    "dataloaders = torch.utils.data.DataLoader(image_datasets['test'], batch_size=1, shuffle=False, num_workers=0)\n",
    "dataset_sizes = {'test': len(image_datasets['test'])}\n",
    "class_names = image_datasets['test'].classes\n",
    "print(dataset_sizes)\n",
    "print(class_names)\n",
    "\n",
    "#setup path where model is stored (pre-trained on kaggle)\n",
    "PATH = './resources/emotion_rec_out/emotionRecognition_restnet.pt'\n",
    "\n",
    "model = torchvision.models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(class_names))\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "model.eval()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9442a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = []\n",
    "predictions = []\n",
    "for sample, label in dataloaders:\n",
    "    sample = sample.to(device)\n",
    "    label = label.cpu().numpy()[0]\n",
    "    Y_test.append(label)\n",
    "    #predict with CNN\n",
    "    outputs = model(sample)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted = predicted.cpu().numpy()[0]\n",
    "    predictions.append(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3d864679",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UAR = 0.5390625\n",
      "WAR = 0.5583333333333333\n",
      "[[20  1  3  3  1  0  0  4]\n",
      " [ 0 16  3  4  0  2  7  0]\n",
      " [ 6  0 25  1  0  0  0  0]\n",
      " [ 3  0  4 20  2  1  2  0]\n",
      " [ 5  6  1  1  8  1  2  8]\n",
      " [ 0  5  2  3  0  4  2  0]\n",
      " [ 2  7  5  8  0  0 10  0]\n",
      " [ 1  0  0  0  0  0  0 31]]\n",
      "Recall for angry = 62.5 %\n",
      "Recall for calm = 50.0 %\n",
      "Recall for disgust = 78.125 %\n",
      "Recall for fearful = 62.5 %\n",
      "Recall for happy = 25.0 %\n",
      "Recall for neutral = 25.0 %\n",
      "Recall for sad = 31.25 %\n",
      "Recall for surprised = 96.875 %\n",
      "Precision for angry = 54.054054054054056 %\n",
      "Precision for calm = 45.714285714285715 %\n",
      "Precision for disgust = 58.139534883720934 %\n",
      "Precision for fearful = 50.0 %\n",
      "Precision for happy = 72.72727272727273 %\n",
      "Precision for neutral = 50.0 %\n",
      "Precision for sad = 43.47826086956522 %\n",
      "Precision for surprised = 72.09302325581395 %\n"
     ]
    }
   ],
   "source": [
    "# Compute evaluation metrics\n",
    "UAR = recall_score(Y_test, predictions, average='macro')\n",
    "WAR = recall_score(Y_test, predictions, average='weighted')\n",
    "print('UAR =', UAR)\n",
    "print('WAR =', WAR)\n",
    "# Confusion matrix\n",
    "# Angst, Ekel, Freude, Langeweile, Neutral, Traurigkeit, Wut\n",
    "cm = confusion_matrix(Y_test, predictions, labels=[0,1,2,3,4,5,6,7])\n",
    "print(cm)\n",
    "\n",
    "# Compute recall, precision, and F1 score for each class\n",
    "rec_result = recall_score(Y_test, predictions, average=None, labels=[0,1,2,3,4,5,6,7])\n",
    "for i, class_ in enumerate(class_names):\n",
    "    print(f'Recall for {class_} =', rec_result[i]*100, '%')\n",
    "\n",
    "\n",
    "prec_result = precision_score(Y_test, predictions, average=None, labels=[0,1,2,3,4,5,6,7])\n",
    "for i, class_ in enumerate(class_names):\n",
    "    print(f'Precision for {class_} =', prec_result[i]*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f03fe",
   "metadata": {},
   "source": [
    "Our trained CNN performs decently, with a WAR of 55% it is a solid 5 percentage points better than the baseline. The WAR is up by around 4 percents. The training was done via Kaggle on grayscale images of the spectograms, we use the trained model for evaluation. Due to the rather limited size of our dataset, our assumption is that the model cannot generalize well and therefore clearly lacks behind our found SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcdca61",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "Task 3: Interpretation of results\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35914f43",
   "metadata": {},
   "source": [
    "The interpretations can both be found directly under seach method + in our presentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
